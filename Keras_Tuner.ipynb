{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Keras-Tuner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemul997/Hyperparams-Optimisation/blob/master/Keras_Tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0ma84T4jpn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "60df11dc-2717-4682-d9dc-0cad3a616f2b"
      },
      "source": [
        "pip install -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/f7/4b41b6832abf4c9bef71a664dc563adb25afc5812831667c6db572b1a261/keras-tuner-1.0.1.tar.gz (54kB)\n",
            "\r\u001b[K     |██████                          | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 1.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.16.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-cp36-none-any.whl size=73200 sha256=e60d67727ce090e70b23f155d815a31a57e0707b1a934edef9dc17e316bd7324\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/cc/62/52716b70dd90f3db12519233c3a93a5360bc672da1a10ded43\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=7d594b882fed3c0da07f6ca5e82de46927780b557380e920472e7c40211f92a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.3 keras-tuner-1.0.1 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "d-0MUKE4ih_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ec87413f-98f2-4421-e7e3-af9fddb4beee"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "import kerastuner as kt\n",
        "\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test  = x_test.reshape(10000, 784)\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "    \n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test  = keras.utils.to_categorical(y_test, num_classes)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2kMB8cBih_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "tuner = kt.Hyperband(create_demo_model,\n",
        "                     objective = 'val_accuracy', \n",
        "                     max_epochs = 20,\n",
        "                     hyperband_iterations = 5,\n",
        "                     factor = 3,\n",
        "                     directory = 'my_dir',\n",
        "                     project_name = 'intro_to_kt') \n",
        "\n",
        "tuner.search(x_train, y_train, epochs = 20, validation_data = (x_test, y_test))\n",
        "\"\"\"                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0DGoo6nih_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 10\n",
        "\n",
        "global_params = {\n",
        "    'layers'      : tuple(range(2, 4)), #Global layers\n",
        "    'batch_size'  : tuple(range(8, 32, 8)),\n",
        "    'epochs'      : tuple(range(1, 5)),\n",
        "    'optimizers'  : ('SGD', 'Adam', 'Adagrad', 'RMSprop', 'Adadelta', 'Adamax', 'Nadam'),\n",
        "    'learning_rate'  : tuple(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 100))\n",
        "}\n",
        "\n",
        "\n",
        "local_params = {\n",
        "\n",
        "    'neurals'     : tuple(range(128, 512, 128)), #Neurals on each layer\n",
        "    'func_act'    : ( 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'softsign', 'hard_sigmoid', 'linear', 'exponential' ),\n",
        "    'func_reg'    : ( 'None', 'l1', 'l2', 'l1_l2' ),\n",
        "    'reg_coef'    :  tuple(np.logspace(np.log10(1e-7), np.log10(1e-4), base = 10, num = 100)),\n",
        "    'weight_init' : ( 'random_normal', 'random_uniform', 'truncated_normal', 'glorot_normal', 'glorot_uniform'\n",
        "                     , 'identity', 'orthogonal' )\n",
        "}\n",
        "\n",
        "def init_regularizer(func_reg, coef_reg):\n",
        "    if (func_reg == 'None'):\n",
        "        return func_reg\n",
        "    \n",
        "    regularizer = 'tf.keras.regularizers.' + func_reg + '(' + str(coef_reg)\n",
        "    \n",
        "    if (func_reg == 'l1_l2') :\n",
        "        regularizer += ', ' + str(coef_reg)\n",
        "        \n",
        "    regularizer += ')'\n",
        "    \n",
        "    return regularizer\n",
        "\n",
        "\n",
        "def create_demo_model(hp):\n",
        "    network = keras.Sequential()      \n",
        "    \n",
        "    for i in range(hp.Choice('num_layers', global_params['layers'])):\n",
        "        \n",
        "        func_reg = hp.Choice('func_reg_' + str(i), local_params['func_reg'])\n",
        "        coef_reg = hp.Choice('reg_coef_' + str(i),  local_params['reg_coef'])\n",
        "        regularizer = init_regularizer(func_reg, coef_reg)\n",
        "        network.add(tf.keras.layers.Dense(units=hp.Choice('units_' + str(i), local_params['neurals']), \n",
        "                                          activation=hp.Choice('func_act_' + str(i), local_params['func_act']),\n",
        "                                          kernel_regularizer=eval(regularizer),\n",
        "                                          kernel_initializer = hp.Choice('weight_init', local_params['weight_init'])))\n",
        "        \n",
        "    network.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    hp_learning_rate = hp.Choice('learning_rate', values = global_params['learning_rate'])\n",
        "    \n",
        "    hp_optimizer = 'tf.keras.optimizers.' + hp.Choice('func_opt', global_params['optimizers']) + '(' + str(hp_learning_rate) + ')';\n",
        "    \n",
        "    network.compile(optimizer = eval(hp_optimizer), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "    return network\n",
        "\n",
        "\n",
        "class CustomHyperband(kt.tuners.Hyperband):\n",
        "    def run_trial(self, trial, *args, **kwargs):\n",
        "        # You can add additional HyperParameters for preprocessing and custom training loops\n",
        "        # via overriding `run_trial`\n",
        "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 32, 256, step=32)\n",
        "        kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 30)\n",
        "        super(CustomHyperband, self).run_trial(trial, *args, **kwargs)\n",
        "    \n",
        "class CustomBayesianOptimisation(kt.tuners.BayesianOptimization):\n",
        "    def run_trial(self, trial, *args, **kwargs):\n",
        "        # You can add additional HyperParameters for preprocessing and custom training loops\n",
        "        # via overriding `run_trial`\n",
        "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 32, 256, step=32)\n",
        "        kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 30)\n",
        "        super(CustomBayesianOptimisation, self).run_trial(trial, *args, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "LycKRxyIih_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperband_tuner = CustomHyperband(create_demo_model,\n",
        "                              objective = 'val_accuracy', \n",
        "                              max_epochs = 20,\n",
        "                              hyperband_iterations = 5,\n",
        "                              factor = 3,\n",
        "                              directory = 'my_dir',\n",
        "                              project_name = 'intro_to_kt')\n",
        "\n",
        "\n",
        "#Max_trials - количество экспериментов\n",
        "bayesian_tuner = CustomBayesianOptimisation(create_demo_model,\n",
        "                                           objective = 'val_accuracy',\n",
        "                                           max_trials = 10,\n",
        "                                           directory = 'my_dir',\n",
        "                                           project_name = 'intro_to_bayesian')\n",
        "\n",
        "\n",
        "#hyperband_tuner.search(x_train, y_train, validation_data = (x_test, y_test)\n",
        "#            , callbacks=[tf.keras.callbacks.EarlyStopping('val_accuracy', patience=3)])\n",
        "\n",
        "bayesian_tuner.search(x_train, y_train, validation_data = (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-rEKCdDih_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperband_tuner = CustomHyperband(create_demo_model,\n",
        "                              objective = 'val_accuracy', \n",
        "                              max_epochs = 20,\n",
        "                              hyperband_iterations = 5,\n",
        "                              factor = 3)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}